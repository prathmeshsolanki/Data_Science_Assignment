{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "\n",
        "Answer:\n",
        "Logistic Regression is a statistical model used for classification problems, especially when the output is binary (0 or 1) — like predicting whether an email is spam or not.\n",
        "It predicts the probability of an event occurring using the Sigmoid function.\n",
        "\n",
        "- Linear Regression predicts continuous values (e.g., house price).\n",
        "\n",
        "- Logistic Regression predicts categorical outcomes (e.g., yes/no).\n",
        "\n",
        "- Linear Regression uses a straight line, while Logistic Regression uses an S-shaped curve (sigmoid) to map values between 0 and 1."
      ],
      "metadata": {
        "id": "HaN3j0N9btyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Answer:\n",
        "The Sigmoid function converts the linear output of the model into a probability value between 0 and 1.\n",
        "It ensures predictions are interpretable as probabilities.\n",
        "\n",
        "If the output (probability) is > 0.5, the model predicts 1 (positive class); otherwise, 0 (negative class)."
      ],
      "metadata": {
        "id": "zrLSXksGbtvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer:\n",
        "Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function.\n",
        "It prevents the model from giving too much importance (high weights) to any one feature.\n",
        "\n",
        "Types:\n",
        "\n",
        "- L1 Regularization (Lasso): Adds the absolute value of weights as penalty — can make some weights zero (feature selection).\n",
        "\n",
        "- L2 Regularization (Ridge): Adds the square of weights as penalty — keeps all features but reduces their impact.\n",
        "\n",
        "Regularization improves the generalization and stability of the model."
      ],
      "metadata": {
        "id": "ZlX_oyNbbtqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer:\n",
        "Evaluation metrics help measure how well a classification model performs. Common metrics include:\n",
        "\n",
        "- Accuracy:\n",
        "\n",
        "Ratio of correctly predicted observations to total observations.\n",
        "\n",
        "Simple but can be misleading if the data is imbalanced.\n",
        "\n",
        "- Precision:\n",
        "\n",
        "Ratio of correctly predicted positive observations to total predicted positives.\n",
        "\n",
        "Useful when false positives are costly.\n",
        "\n",
        "- Recall (Sensitivity):\n",
        "\n",
        "Ratio of correctly predicted positives to all actual positives.\n",
        "\n",
        "Useful when missing positive cases is costly.\n",
        "\n",
        "- F1-Score:\n",
        "\n",
        "Harmonic mean of precision and recall.\n",
        "\n",
        "Balances both precision and recall.\n",
        "\n",
        "- Confusion Matrix:\n",
        "\n",
        "A table showing counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Helps visualize model performance.\n",
        "\n",
        "- ROC-AUC Score:\n",
        "\n",
        "Measures how well the model separates the classes.\n",
        "\n",
        "A higher value (closer to 1) means better performance.\n",
        "\n",
        "These metrics are important because they provide different perspectives on model performance and help you choose the best model for your specific problem."
      ],
      "metadata": {
        "id": "lUoRwHdUbtoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "\n",
        "Answer:\n",
        "\n",
        "                                   "
      ],
      "metadata": {
        "id": "MYmHvVR7btfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag8QzlRkeTiP",
        "outputId": "9b977a7a-f34b-4b0c-ee60-ee119fcba553"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy. (Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "YwMbqYAOef7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with L2 Regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print coefficients and accuracy\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_6_RSveUQI",
        "outputId": "5d945d03-9ae0-4273-978d-49c41b836e42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients: [[ 2.09981182  0.13248576 -0.10346836 -0.00255646 -0.17024348 -0.37984365\n",
            "  -0.69120719 -0.4081069  -0.23506963 -0.02356426 -0.0854046   1.12246945\n",
            "  -0.32575716 -0.06519356 -0.02371113  0.05960156  0.00452206 -0.04277587\n",
            "  -0.04148042  0.01425051  0.96630267 -0.37712622 -0.05858253 -0.02395975\n",
            "  -0.31765956 -1.00443507 -1.57134711 -0.69351401 -0.84095566 -0.09308282]]\n",
            "Model Intercept: [2.13128402]\n",
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "vgJb3Z3_e8AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load multiclass dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_82s5Hzeu6H",
        "outputId": "d8ab03a6-97c0-4ec2-aaec-c6aa71e47b0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "vPApJ9TMfIOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and evaluate\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Validation Accuracy:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWpyiHcWe-jL",
        "outputId": "334e8035-8d6a-4ed6-9518-f08ece83e2b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Validation Accuracy: 0.9626373626373628\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling. (Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "dMB3X2pXfRDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "model1.fit(X_train, y_train)\n",
        "y_pred1 = model1.predict(X_test)\n",
        "acc_without_scaling = accuracy_score(y_test, y_pred1)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "y_pred2 = model2.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "# Compare results\n",
        "print(\"Accuracy without Scaling:\", acc_without_scaling)\n",
        "print(\"Accuracy with Scaling:\", acc_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqgsNXNIfQNm",
        "outputId": "f6a97cab-aaab-46fa-e746-b7ada0ead223"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9736842105263158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Data Understanding and Cleaning\n",
        "\n",
        "- Load the data and explore it using summary statistics (df.describe(), df.info()).\n",
        "\n",
        "    >  Handle missing values:\n",
        "\n",
        "     -   Impute numerical features using mean/median.\n",
        "\n",
        "     -   Impute categorical features using mode or create a separate category “Unknown.”\n",
        "\n",
        "- Remove duplicates and irrelevant features (like user IDs or timestamps not adding value).\n",
        "\n",
        "\n",
        "\n",
        "2. Feature Engineering\n",
        "\n",
        "- Convert categorical variables using One-Hot Encoding or Label Encoding.\n",
        "\n",
        "> Create new meaningful features like:\n",
        "\n",
        "- Customer purchase frequency, total spending, time since last purchase, etc.\n",
        "\n",
        "- Drop highly correlated or redundant features to avoid multicollinearity.\n",
        "\n",
        "3. Feature Scaling\n",
        "\n",
        "- Use StandardScaler or MinMaxScaler to standardize numerical features.\n",
        "\n",
        "- Scaling helps the Logistic Regression algorithm converge faster and improves stability.\n",
        "\n",
        "4. Handling Class Imbalance (Only 5% respond)\n",
        "\n",
        "- Since the dataset is highly imbalanced, accuracy alone is misleading.\n",
        "Use one or a combination of these techniques:\n",
        "\n",
        "> Resampling Techniques:\n",
        "\n",
        "- Oversampling minority class using SMOTE (Synthetic Minority Oversampling Technique).\n",
        "\n",
        "- Undersampling majority class to balance proportions.\n",
        "\n",
        "> Class Weight Adjustment:\n",
        "\n",
        "- Set class_weight='balanced' in Logistic Regression to automatically adjust for imbalance.\n",
        "\n",
        "- This penalizes misclassification of minority class more heavily.\n",
        "\n",
        "5. Model Training\n",
        "\n",
        "- Train a Logistic Regression model:\n",
        "\n",
        "             model = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000)\n",
        "\n",
        "\n",
        "- Use cross-validation (e.g., StratifiedKFold) to ensure fair representation of both classes in each fold.\n",
        "\n",
        "6. Hyperparameter Tuning\n",
        "\n",
        "- Use GridSearchCV to tune parameters like:\n",
        "\n",
        "- C (regularization strength)\n",
        "\n",
        "- penalty (L1 or L2)\n",
        "\n",
        "- solver (liblinear, saga)\n",
        "\n",
        "- This helps find the best balance between bias and variance.\n",
        "\n",
        "7. Model Evaluation\n",
        "\n",
        "- Because the data is imbalanced, focus on metrics beyond accuracy:\n",
        "\n",
        "- Precision: How many predicted responders are actually correct.\n",
        "\n",
        "- Recall: How many actual responders we identified correctly.\n",
        "\n",
        "- F1-Score: Balances precision and recall.\n",
        "\n",
        "- ROC-AUC Score: Measures ability to distinguish between responders and non-responders.\n",
        "\n",
        "- Confusion Matrix: Gives insight into true positives and false negatives.\n",
        "\n",
        "- For business cases like marketing response prediction, Recall and AUC are more important than Accuracy — missing a potential responder is more costly.\n",
        "\n",
        "8. Model Interpretation and Business Application\n",
        "\n",
        "- Examine model coefficients to understand which features most influence campaign response.\n",
        "\n",
        "- Share insights with the marketing team to target high-probability customers.\n",
        "\n",
        "- Continuously retrain and monitor the model as customer behavior changes.\n"
      ],
      "metadata": {
        "id": "b76oIo19fhQu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVJ7WuwifXyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}