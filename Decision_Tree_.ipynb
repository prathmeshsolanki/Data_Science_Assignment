{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "In classification, it predicts the class label of an instance by learning simple decision rules from data features.\n",
        "\n",
        "- The dataset is split into smaller subsets based on feature values.\n",
        "\n",
        "- At each internal node, a feature is chosen that best divides the data into classes.\n",
        "\n",
        "- Each branch represents a possible outcome, and each leaf node represents a final class label (decision).\n",
        "\n",
        "*How it works (Classification Example):*\n",
        "\n",
        "1. Start with the root node containing all training data.\n",
        "\n",
        "2. For each feature, calculate an impurity measure (like Gini or Entropy).\n",
        "\n",
        "3. Select the feature that gives the best split (lowest impurity or highest information gain).\n",
        "\n",
        "4. Repeat the process for each child node until:\n",
        "\n",
        "    - All samples in a node belong to the same class, or\n",
        "\n",
        "    - No more features are left.\n",
        "\n",
        "    - This forms a tree-like structure of decisions."
      ],
      "metadata": {
        "id": "LSzND-lrrRqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:\n",
        "Both Gini Impurity and Entropy measure how mixed (impure) the classes are in a node.\n",
        "\n",
        "ðŸ§® Gini Impurity:\n",
        "            \n",
        "            Gini=1âˆ’âˆ‘(piâ€‹)2\n",
        "\n",
        "Where:\n",
        "pi = probability of class i in a node.\n",
        "\n",
        "- Gini = 0 â†’ Node is pure (only one class).\n",
        "\n",
        "- Gini is higher when classes are mixed.\n",
        "\n",
        "- Used in CART (Classification and Regression Trees).\n",
        "\n",
        "ðŸ§® Entropy:\n",
        "\n",
        "            Entropy=âˆ’âˆ‘piâ€‹log2â€‹(piâ€‹)\n",
        "\n",
        "- Entropy = 0 â†’ Node is pure.\n",
        "\n",
        "- Entropy increases as the mixture of classes increases.\n",
        "\n",
        "- Used in ID3 and C4.5 algorithms.\n",
        "\n",
        "> Impact on Splits:\n",
        "\n",
        "- Both measures help decide which feature to split on.\n",
        "\n",
        "- The algorithm chooses the feature that reduces impurity the most (maximizes Information Gain).\n",
        "\n",
        "- Lower impurity â†’ better split â†’ more accurate classification."
      ],
      "metadata": {
        "id": "fbC6TIjGrvtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "| Type                             | Description                                                                                  | Practical Advantage                                                                      |\n",
        "| -------------------------------- | -------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
        "| **Pre-Pruning (Early Stopping)** | Stops growing the tree early based on conditions (e.g., max depth, min samples per leaf).    | **Faster training** and prevents overfitting early.                                      |\n",
        "| **Post-Pruning**                 | First grow a full tree, then remove branches that donâ€™t improve accuracy on validation data. | **Better generalization** â€” the model is simplified after training to avoid overfitting. |\n",
        "\n"
      ],
      "metadata": {
        "id": "PfmLkxS3rRm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "Information Gain (IG) measures how much uncertainty (impurity) is reduced after splitting a dataset based on a feature.\n",
        "\n",
        "            IG=Entropy(Parent)âˆ’âˆ‘NNiâ€‹â€‹Ã—Entropy(Childiâ€‹)\n",
        "\n",
        "Where:\n",
        "\n",
        "Ni: number of samples in child node\n",
        "\n",
        "N: total samples in parent node\n",
        "\n",
        "Importance:\n",
        "\n",
        "It helps select the best feature for splitting the data.\n",
        "\n",
        "A feature with high Information Gain means it creates more homogeneous (pure) groups â€” leading to better classification accuracy."
      ],
      "metadata": {
        "id": "96AWw0-prRjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "Real-World Applications:\n",
        "\n",
        "1. Healthcare:\n",
        "\n",
        "- Used to predict diseases (e.g., diabetes, heart disease) based on patient data.\n",
        "\n",
        "- Example: Classifying patients as High Risk or Low Risk using medical test results.\n",
        "\n",
        "2. Finance:\n",
        "\n",
        "- Used for credit risk analysis â€” deciding whether to approve a loan or not.\n",
        "\n",
        "- Helps detect fraudulent transactions.\n",
        "\n",
        "3. Marketing:\n",
        "\n",
        "- Customer segmentation and predicting purchase likelihood.\n",
        "\n",
        "- Example: Predicting if a customer will respond to a campaign.\n",
        "\n",
        "4. Education:\n",
        "\n",
        "- Predicting student performance or dropout risk based on attendance, grades, etc.\n",
        "\n",
        "5. Manufacturing / Quality Control:\n",
        "\n",
        "- Used to identify causes of product defects by analyzing production parameters.\n",
        "\n",
        "6. Agriculture / Environment:\n",
        "\n",
        "- Classifying plant species using features (like in the Iris dataset).\n",
        "\n",
        "- Predicting pollution levels, rainfall, or crop yield.\n",
        "\n",
        "7. Real Estate:\n",
        "\n",
        "- Predicting house prices using factors like number of rooms, area, and location (like Boston Housing dataset).\n",
        "\n",
        "\n",
        "> Advantages of Decision Trees:\n",
        "\n",
        "1.Easy to Understand and Visualize â€“ Mimics human decision-making.\n",
        "\n",
        "2.No Feature Scaling Needed â€“ Works without normalization or standardization.\n",
        "\n",
        "3.Handles Both Numerical & Categorical Data.\n",
        "\n",
        "4.Can Capture Non-linear Relationships.\n",
        "\n",
        "5.Fast Prediction once the tree is built.\n",
        "\n",
        "\n",
        "\n",
        ">Limitations of Decision Trees:\n",
        "\n",
        "1. Overfitting:\n",
        "\n",
        "- Trees can grow too deep and memorize training data.(Handled by pruning or limiting depth.)\n",
        "\n",
        "2. Unstable:\n",
        "\n",
        "- Small data changes can cause a completely different tree.\n",
        "\n",
        "3. Biased Towards Dominant Classes:\n",
        "\n",
        "- If dataset is imbalanced, tree may favor majority class.\n",
        "\n",
        "4. Not Great for Continuous Predictions Alone:\n",
        "\n",
        "- Regression trees can give piecewise constant outputs.\n",
        "\n",
        "- Ensemble methods (like Random Forest or Gradient Boosting) perform better.\n",
        "\n",
        "Example Datasets:\n",
        "\n",
        "1. Iris Dataset (Classification):\n",
        "\n",
        "- Task: Predict flower species (Setosa, Versicolor, Virginica) using features like sepal and petal length/width.\n",
        "\n",
        "- from sklearn.datasets import load_iris\n",
        "\n",
        "2. Boston Housing Dataset (Regression):\n",
        "\n",
        "- Task: Predict median house prices based on features like number of rooms, age, and location.\n",
        "\n",
        "- from sklearn.datasets import load_boston (deprecated; use updated housing datasets in sklearn or fetch_openml instead)"
      ],
      "metadata": {
        "id": "riCulgGrrRgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the modelâ€™s accuracy and feature importances (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "7S1WVjkyrRcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6 Answer\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train Decision Tree Classifier using Gini criterion\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf_gini.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Classifier (Gini) Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf_gini.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwgdCfOXwEfY",
        "outputId": "f30b4cc1-6edf-4ec3-80bf-1f1b08197e64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (Gini) Accuracy: 100.0 %\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "- Load the Iris Dataset\n",
        "\n",
        "- Train a Decision Tree Classifier with max_depth=3\n",
        "\n",
        "- Compare its accuracy to a fully-grown tree\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Ty76hj5nrRaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7 Answer\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Print results\n",
        "print(\"Fully-Grown Tree Accuracy:\", round(accuracy_full * 100, 2), \"%\")\n",
        "print(\"Tree with max_depth=3 Accuracy:\", round(accuracy_limited * 100, 2), \"%\")\n",
        "\n",
        "# Compare\n",
        "if accuracy_limited < accuracy_full:\n",
        "    print(\"\\nThe limited-depth tree is slightly less accurate but generalizes better.\")\n",
        "else:\n",
        "    print(\"\\nBoth trees have similar accuracy; limiting depth helps prevent overfitting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVTITIGw0y9",
        "outputId": "ef9f7152-5896-46f0-fad5-a1056d7d0e2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 100.0 %\n",
            "Tree with max_depth=3 Accuracy: 100.0 %\n",
            "\n",
            "Both trees have similar accuracy; limiting depth helps prevent overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8:\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "- Load the Boston Housing Dataset\n",
        "\n",
        "- Train a Decision Tree Regressor\n",
        "\n",
        "- Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "MercoRX6rRVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8 Answer\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset (fetch_openml replaces deprecated load_boston)\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Regressor - Mean Squared Error (MSE):\", round(mse, 2))\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02xD30LixM3_",
        "outputId": "1801490c-1742-4bb0-ec39-8d54f98ad33c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor - Mean Squared Error (MSE): 11.59\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9:\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "- Load the Iris Dataset\n",
        "\n",
        "- Tune the Decision Treeâ€™s max_depth and min_samples_split using GridSearchCV\n",
        "\n",
        "- Print the best parameters and model accuracy\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "HRFFxliKrRT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9 Answer\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters from GridSearchCV:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkDIuABgxcjs",
        "outputId": "61358757-f33f-461f-92be-aac1b8c7a271"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters from GridSearchCV: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine youâ€™re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "IOeuO8UOrRPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Handle missing values\n",
        "\n",
        "- Diagnose type of missingness: MCAR (random), MAR (depends on other observed features), MNAR (depends on unobserved value). This affects strategy.\n",
        "\n",
        "> Simple rules:\n",
        "\n",
        "- If a column has very high missing rate (e.g., >50â€“70%), consider dropping it unless clinically important.\n",
        "\n",
        "> For numerical features:\n",
        "\n",
        "- If MCAR/MAR: impute with median (robust) or use model-based imputation (KNN, IterativeImputer) if relationships exist.\n",
        "\n",
        "- If missingness itself is informative (e.g., test not ordered because doctor thought patient healthy) â€” add a binary indicator column feature_X_was_missing.\n",
        "\n",
        "> For categorical features:\n",
        "\n",
        "- Impute missing as a separate category \"<missing>\" or the most frequent category.\n",
        "\n",
        "- Avoid leaking target info: Always fit imputers on training data only (use pipelines).\n",
        "\n",
        "    - Advanced: If many missing patterns and correlated missingness, consider IterativeImputer or models that can handle missingness natively.\n",
        "\n",
        "\n",
        "2) Encode categorical features\n",
        "\n",
        "- Low-cardinality categorical (few unique values): OneHotEncoder (sparse or drop one col to avoid collinearity).\n",
        "\n",
        "- High-cardinality categorical: target encoding or frequency encoding (careful to prevent leakageâ€”use CV folds for target encoding).\n",
        "\n",
        "- Ordinal features: use OrdinalEncoder if order matters.\n",
        "\n",
        "- In pipelines: use ColumnTransformer to apply different transforms to numeric vs categorical features.\n",
        "\n",
        "- Remember: Decision Trees donâ€™t require scaling, but proper encoding still matters.\n",
        "\n",
        "\n",
        "3) Train a Decision Tree model\n",
        "\n",
        "- Use scikit-learn DecisionTreeClassifier inside a pipeline:\n",
        "\n",
        "- Keeps imputation + encoding + model in one reproducible unit.\n",
        "\n",
        "- Set random_state for reproducibility.\n",
        "\n",
        "- Consider class_weight='balanced' (or custom weights) if dataset is imbalanced.\n",
        "\n",
        "- Example pipeline (compact, runnable):"
      ],
      "metadata": {
        "id": "Fu86gT2zyDUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# X, y already loaded (pandas)\n",
        "num_cols = ['age','blood_pressure','cholesterol']    # example\n",
        "cat_cols = ['gender','smoking_status']\n",
        "\n",
        "num_transform = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    # scaler optional for tree but ok to include if later using other models\n",
        "])\n",
        "\n",
        "cat_transform = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transform, num_cols),\n",
        "    ('cat', cat_transform, cat_cols)\n",
        "])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Q9Fo8N-mxwlw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Tune hyperparameters\n",
        "\n",
        "- Hyperparameters that matter for Decision Trees:\n",
        "\n",
        "    - max_depth â€” controls complexity.\n",
        "\n",
        "    - min_samples_split, min_samples_leaf â€” prevent tiny leaves.\n",
        "\n",
        "    - max_features â€” how many features to consider at split.\n",
        "\n",
        "    - criterion â€” 'gini' or 'entropy'.\n",
        "\n",
        "- Use GridSearchCV or RandomizedSearchCV with cv=5 (stratified if classification).\n",
        "\n",
        "- Use scoring aligned with business need: e.g., scoring='recall' (if missing a disease is costly) or scoring='roc_auc'.\n",
        "\n",
        "- Example grid:\n",
        "\n",
        "              param_grid = {\n",
        "                            'clf__max_depth': [3, 5, 8, None],\n",
        "                            'clf__min_samples_split': [2, 5, 10],\n",
        "                            'clf__min_samples_leaf': [1, 2, 5]\n",
        "                        }\n",
        "\n",
        "                        grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "                        grid.fit(X_train, y_train)\n",
        "                        best_model = grid.best_estimator_\n",
        "                        print(grid.best_params_)\n",
        "\n",
        "- If classes are very imbalanced, also try:\n",
        "\n",
        "    - class_weight='balanced' or custom weights,\n",
        "\n",
        "    - resampling: SMOTE (only in training folds) or undersampling majority class."
      ],
      "metadata": {
        "id": "m2aVLeDjy-wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Evaluate performance\n",
        "\n",
        "- Prefer multiple metrics:\n",
        "\n",
        "    - Recall (Sensitivity) â€” proportion of actual patients with disease that model finds. Crucial if missing a disease is bad.\n",
        "\n",
        "    - Precision â€” how many predicted positives are true positives (important when false positives are costly).\n",
        "\n",
        "    - F1-score â€” balance between precision and recall.\n",
        "\n",
        "    - ROC AUC â€” ranking ability across thresholds.\n",
        "\n",
        "    - PR AUC â€” more informative on imbalanced data.\n",
        "\n",
        "    - Confusion Matrix â€” absolute counts of TP/FP/FN/TN for operational decisions.\n",
        "\n",
        "    - Calibration â€” does predicted probability match true probability? Use calibration plots or CalibratedClassifierCV.\n",
        "\n",
        "- Decision thresholds: choose threshold not necessarily 0.5. For health, you might use lower threshold to increase recall and accept more false positives.\n",
        "\n",
        "- Cross-validation: Report CV meanÂ±std for metrics. Use nested CV for honest hyperparameter selection if reporting final performance.\n",
        "\n",
        "- Statistical and clinical validation: evaluate on holdout test set and, ideally, on an external dataset from different hospital/population."
      ],
      "metadata": {
        "id": "9IFbuKt_zdcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Interpretability & fairness\n",
        "\n",
        "- Feature importances from tree give a quick sense which features matter.\n",
        "\n",
        "- For more robust explanations: use SHAP or LIME to show per-patient explanations.\n",
        "\n",
        "- Check for bias across subgroups (age, gender, ethnicity) â€” ensure model doesn't systematically underperform on any group.\n",
        "\n",
        "- Keep clinical experts involved â€” they can validate whether learned rules make clinical sense."
      ],
      "metadata": {
        "id": "wrPNz9200Jyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Deployment, monitoring & governance\n",
        "\n",
        "  - Clinical integration: model as decision support, not fully autonomous â€” flag high-risk patients for clinician review.\n",
        "\n",
        "  - Monitor model drift (data distribution changes), performance decay, and changes in prevalence.\n",
        "\n",
        "  - Logging: save model inputs/outputs for auditing.\n",
        "\n",
        "  - Regulatory & privacy: ensure HIPAA/GDPR compliance, secure data handling, and maintain documentation for explainability and approvals."
      ],
      "metadata": {
        "id": "5Iny24CT0RTP"
      }
    }
  ]
}